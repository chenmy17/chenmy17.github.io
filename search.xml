<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[经典卷积神经网络模型：LeNet-5]]></title>
    <url>%2F2018%2F03%2F26%2F%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%EF%BC%9ALeNet-5%2F</url>
    <content type="text"><![CDATA[经典卷积神经网络模型：LeNet-5最近在看CaiCloud的TensorFlow框架这本书，刚好看到卷积这一章，顺便做个笔记，以免边看边忘🌝（￣。。￣） 卷积神经网络在图像数据集上有很突出的表现。与全连接神经网络不同的是，卷积神经网络的相邻两层的节点并不是都有连接。 👆图就是一个卷积神经网络的基本架构图。图中可以看出，卷积神经网络前几层的每一个节点都只与上一层的部分节点相连。一个神经网络主要由👇五种结构组成： 输入层 卷积层 池化层 全连接层 softmax层 以LeNet-5的TensorFlow实现为例了解每一层的作用的具体功能。 神经网络的输入因为LeNet-5解决的是MNIST数字识别问题，所以输入是原始图像的像素28×28×1。卷积神经网络的输入层为一个三维矩阵，所以输入的数据格式如下： 12345678x = tf.placeholder(tf.float32,[ #第一维表示一个batch中样例的个数 BATCH_SIZE, mnist_inference.IMAGE_SIZE, mnist_inference.IMAGE_SIZE, #图片的深度，对于RGB图像为3 mnist_inference.NUM_CHANNEL], name = &apos;x-input&apos;) 神经网络的参数12345678910111213141516171819import tensorflow as tf#配置神经网络的参数INPUT_NODE = 784OUTPUT_NODE = 10IMAGE_SIZE = 28NUM_CHANNELS = 1NUM_LABELS = 10#第一层卷积的尺寸和深度CONV1_DEEP = 32CONV1_SIZE = 5#第二层卷积的尺寸和深度CONV2_DEEP = 64CONV2_SIZE = 5#全连接层节点的个数FC_SIZE = 512 卷积网络前向传播过程layer1-conv1 卷积层12345678def inferecne(input_tensor,train,regularizer): with tf.variabel_scope(&apos;layer1-conv1&apos;): conv1_weights = tf.get_variable( &quot;weight&quot;,[CONV1_SIZE,CONV1_SIZE,NUM_CHANNELS,CONV1_DEEP], initializer=tf.truncated_normal_initializer(stddev=0.1)) conv1_biases = tf.get_variable(&quot;bias&quot;,[CONV1_DEEP],initializer=tf.constant_initializer(0.0) conv1 = tf.nn.conv2d(input_tensor,conv1_weights,strides=[1,1,1,1],padding=&apos;SAME&apos;) relu1 = tf.nn.relu(tf.nn.bias_add(conv1,conv1_biases)) layer2-pool1 最大池化层12with tf.name_variable(&apos;layer2-pool1&apos;) pool1 = tf.nn.max_pool(relu1,ksize=[1,2,2,1],strides=[1,2,2,1],padding=&apos;SAME&apos;) layer3-conv2 第二层卷积层1234567with tf.variable_scope(&apos;layer3-conv2&apos;): conv2_weights = tf.get_variable( &quot;weight&quot;,[CONV2_SIZE,CONV2_SIZE,CONV2_DEEP,CONV2_DEEP], initializer=tf.truncated_normal_initializer(stddev=0.1)) conv2_biases = tf.get_variable(&quot;bias&quot;,[CONV1_DEEP],initializer=tf.constant_initializer(0.0) conv2 = tf.nn.conv2d(pool1,conv2_weights,strides=[1,1,1,1],padding=&apos;SAME&apos;) relu2 = tf.nn.relu(tf.nn.bias_add(conv2,conv2_biases)) layer4-pool2 池化层12with tf.name_scope(&apos;layer4-pool1&apos;): pool2 = tf.nn.max_pool(relu2,ksize=[1,2,2,1],strides=[1,2,2,1],padding=&apos;SAME&apos;) 对数据进行处理进入全连接层1234pool_shape = pool2.get_shape().as_list()nodes = pool_shape[1]*pool_shape[2]*pool_shape[3]#pool_shape[0]中为一个batch中数据的个数reshaped =tf.reshape(pool2,[pool_shape[0],nodes) layer5-fc1全连接层123456789#在这一层中加入了dropout策略避免了过拟合的问题with tf.variable_scope(&apos;layer5-fc1&apos;): fc1_weights = tf.get_variable(&quot;weight&quot;,[nodes,FC_SIZE],initializer=tf.truncated_normal_initializer(stddev=0.1)) if regularizer!=None: tf.add_to_collection(&apos;losses&apos;,regularizer(fc1_weights)) fc1_biases = tf.get_variable(&quot;bias&quot;,[FC_SIZE],initializer=tf.constant_initializer(0.1)) fc1 = tf.nn.relu(tf.matmul(reshaped,fc1_weights)+fc1_biases) if train: fc1 = tf.nn.dropout(fc1,0.5) layer6-fc2全连接层123456789#这一层的输入为512的向量，输出为10的向量with tf.variable_scope(&apos;layer6-fc2&apos;): fc2_weights = tf.get_variable(&quot;weight&quot;,[FC_SIZE,NUM_LABELS],initializer=tf.truncated_normal_initializer(stddev=0.1)) if regularizer!=None: tf.add_to_collection(&apos;losses&apos;,regularizer(fc2_weights)) fc2_biases = tf.get_variable(&quot;bias&quot;,[NUM_LABELS],initializer=tf.constant_initializer(0.1)) logit = tf.matmul(fc1,fc2_weights)+fc2_biases#返回第六层输出return logit]]></content>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow中变量共享的问题]]></title>
    <url>%2F2018%2F03%2F24%2FTensorflow%E4%B8%AD%E5%8F%98%E9%87%8F%E5%85%B1%E4%BA%AB%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Tensorflow中变量共享的问题在TensorFlow中提供了通过变量名来创建或者获取一个变量的机制，通过这个机制在不同的函数中可以直接通过名字来使用变量，而不需要将变量以形参的形式传递给函数。Tensorflow提供了三种创建变量的方式，tf.placeholder,tf.Variable,tf.get_variable三种方式。 三种方式定义变量试着在TensorFlow中分别用三种方式定义变量，观察它们的区别。 tf.placeholder()方式123456789import tensorflow as tfv1 = tf.placeholder(tf.float32, shape=[1,2,3])print v1.namev1 = tf.placeholder(tf.float32, shape=[1,2,3], name=&apos;ts&apos;)print v1.namev1 = tf.placeholder(tf.float32, shape=[1,2,3], name=&apos;ts&apos;)print v1.nameprint type(v1)print v1 12345Placeholder:0ts:0ts_1:0&lt;class &apos;tensorflow.python.framework.ops.Tensor&apos;&gt;Tensor(&quot;ts_1:0&quot;, shape=(1, 2, 3), dtype=float32) tf.Variable()方式123456789import tensorflow as tfv2 = tf.Variable([1,2], dtype=tf.float32)print v2.namev2 = tf.Variable([1,2], dtype=tf.float32, name=&apos;Y&apos;)print v2.namev2 = tf.Variable([1,2], dtype=tf.float32, name=&apos;Y&apos;)print v2.nameprint type(v2)print v2 12345Variable:0Y:0Y_1:0&lt;class &apos;tensorflow.python.ops.variables.Variable&apos;&gt;&lt;tf.Variable &apos;Y_1:0&apos; shape=(2,) dtype=float32_ref&gt; tf.get_variable()方式1234567# 对于tf.get_variable()方法来说，name是一个必填的参数v3 = tf.get_variable(name = &apos;fl&apos;, shape=[])print v3.namev4 = tf.get_variable(name = &apos;fl&apos;, shape=[2])print v4.nameprint type(v3)print v3 1234567fl:0#ValueErrorValueError: Variable fl already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:...#此处报了一个Value错误&lt;class &apos;tensorflow.python.ops.variables.Variable&apos;&gt;&lt;tf.Variable &apos;fl:0&apos; shape=() dtype=float32_ref&gt; 再来做一个小小的实验 12345#由上述的实验可以看出name=&apos;fl&apos;的变量和v3冲突，所以报错，#此处v3,v4都是通过tf.get_variable()方法赋值所以产生冲突。v4 = tf.get_variable(name=&apos;Y&apos;, shape=[1])print type(v4)print v4 12&lt;class &apos;tensorflow.python.ops.variables.Variable&apos;&gt;&lt;tf.Variable &apos;Y_2:0&apos; shape=() dtype=float32_ref&gt; 发现，’Y’变量名虽然已经被占用了，但是在TensorFlow中并不会对非tf.get_variable()方法产生命名冲突。下面就用tf.trainable_variables()函数查看工作区的变量。tf.trainable_variables()可以将所有trainable=True的变量以list的形式返回。 12345vs = tf.trainable_variables()#此处的vs是一个listprint len(vs)for v in vs: print v 1234565&lt;tf.Variable &apos;Variable:0&apos; shape=(2,) dtype=float32_ref&gt;&lt;tf.Variable &apos;Y:0&apos; shape=(2,) dtype=float32_ref&gt;&lt;tf.Variable &apos;Y_1:0&apos; shape=(2,) dtype=float32_ref&gt;&lt;tf.Variable &apos;fl:0&apos; shape=() dtype=float32_ref&gt;&lt;tf.Variable &apos;Y_2:0&apos; shape=() dtype=float32_ref&gt; 结论👆的实验可以得出一个简单的结论，就是只有通过tf.get_variable()方式创建的变量会发生命名冲突，并且tf.get_variable()与其他变量创建的方式也不会发生命名冲突。简单总结一下三种变量命名的方式。 tf.placeholder()trainable=False tf.Variable()trainable=True/False tf.get_variable()一般与tf.variable_scope()一起使用来实现神经网络中的变量共享问题。 命名空间和变量空间TensorFlow中有两种命名空间，name_scope和variable_scope 123456789import tensorflow as tfwith tf.name_scope(&apos;nspace&apos;): v1 = tf.Variable([1], name=&apos;v1&apos;) with tf.variable_scope(&apos;vspace&apos;): v2 = tf.Variable([1], name=&apos;v2&apos;) v3 = tf.get_variable(name=&apos;v3&apos;,shape=[])print &apos;v1.name:&apos;, v1.nameprint &apos;v2.name:&apos;, v2.nameprint &apos;v3.name:&apos;, v3.name 123v1.name: nspace/v1:0v2.name: nspace/vspace/v2:0v3.name: vspace/v3:0 可以看到用tf.get_variable()方法创建的变量不会受tf.name_scope()命名空间的影响。而tf.get_variable()和tf.variable_scope()一起实现了变量的共享。 tf.Variable()创建变量123456789101112131415161718#首先清空当前工作区的变量import tensorflow as tfdef my_image_filter(): conv1_weights = tf.Variable(tf.random_normal([5,5,32,32]),name=&apos;conv1_weights&apos;) conv1_biase = tf.Variable(tf.zeros([32]),name=&apos;conv1_biases&apos;) conv2_weights = tf.Variable(tf.random_normal([5,5,32,32]),name=&apos;conv2_weights&apos;) conv2_biases = tf.Variable(tf.zeros([32]),name=&apos;conv2_biases&apos;) return None #First callresult1 = my_image_filter()#Another callresult2 = my_image_filter()#返回所有可训练的变量到vs列表中vs = tf.trainable_variables()print &apos;There are %d trainable_variables in the Graph:&apos; % len(vs)for v in vs: print v 123456789There are 8 trainable_variables in the Graph:&lt;tf.Variable &apos;conv1_weights:0&apos; shape=(5, 5, 32, 32) dtype=float32_ref&gt;&lt;tf.Variable &apos;conv1_biases:0&apos; shape=(32,) dtype=float32_ref&gt;&lt;tf.Variable &apos;conv2_weights:0&apos; shape=(5, 5, 32, 32) dtype=float32_ref&gt;&lt;tf.Variable &apos;conv2_biases:0&apos; shape=(32,) dtype=float32_ref&gt;&lt;tf.Variable &apos;conv1_weights_1:0&apos; shape=(5, 5, 32, 32) dtype=float32_ref&gt;&lt;tf.Variable &apos;conv1_biases_1:0&apos; shape=(32,) dtype=float32_ref&gt;&lt;tf.Variable &apos;conv2_weights_1:0&apos; shape=(5, 5, 32, 32) dtype=float32_ref&gt;&lt;tf.Variable &apos;conv2_biases_1:0&apos; shape=(32,) dtype=float32_ref&gt; 工作区内有8个变量，因为调用了两次my_image_filter()函数，每调用一次都会重新创建4个变量。 用tf.get_variable()创建变量123456789101112131415161718192021222324252627import tensorflow as tfdef conv_relu(kernel_shape, bias_shape): # Create variable named &quot;weights&quot;. weights = tf.get_variable(&quot;weights&quot;, kernel_shape, initializer=tf.random_normal_initializer()) # Create variable named &quot;biases&quot;. biases = tf.get_variable(&quot;biases&quot;, bias_shape, initializer=tf.constant_initializer(0.0)) return Nonedef my_image_filter(): with tf.variable_scope(&quot;conv1&quot;): # Variables created here will be named &quot;conv1/weights&quot;, &quot;conv1/biases&quot;. relu1 = conv_relu([5, 5, 32, 32], [32]) with tf.variable_scope(&quot;conv2&quot;): # Variables created here will be named &quot;conv2/weights&quot;, &quot;conv2/biases&quot;. return conv_relu( [5, 5, 32, 32], [32])with tf.variable_scope(&quot;image_filters&quot;) as scope: result1 = my_image_filter() scope.reuse_variables() result2 = my_image_filter()vs = tf.trainable_variables()print &apos;There are %d train_able_variables in the Graph: &apos; % len(vs)for v in vs: print v 12345There are 4 train_able_variables in the Graph: &lt;tf.Variable &apos;image_filters/conv1/weights:0&apos; shape=(5, 5, 32, 32) dtype=float32_ref&gt;&lt;tf.Variable &apos;image_filters/conv1/biases:0&apos; shape=(32,) dtype=float32_ref&gt;&lt;tf.Variable &apos;image_filters/conv2/weights:0&apos; shape=(5, 5, 32, 32) dtype=float32_ref&gt;&lt;tf.Variable &apos;image_filters/conv2/biases:0&apos; shape=(32,) dtype=float32_ref&gt; 👆的程序虽然调用了两次函数，但是引用了变量变量的机制，我们只创建了一次变量。实现了变量共享。]]></content>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Markdown中使用图床的示例]]></title>
    <url>%2F2018%2F03%2F15%2F%E5%9C%A8Markdown%E4%B8%AD%E4%BD%BF%E7%94%A8%E5%9B%BE%E5%BA%8A%E7%9A%84%E7%A4%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[markdown中图床使用示例 hhhh有点夸张了，在markdown中添加图片其实很简单，![Markdown](Image URL)就可以了。 有两种方法可以往博客中添加图片，一种是将图片放到本地的source文件中，但是这种当图片过多或过大时会占据大量存储空间导致网站加载缓慢。这个时候就要用到图床这个工具。 图床就是存储图片的服务器，上传本地图片会生成一个URL链接，把链接粘贴到md文件中即可。这里我使用的是贴图库，www.tietuku.com。还有很多这种免费的图床，选择一个自己喜欢的即可。 P.S.早上还说实验室没事晚上就来了，接下来应该会好好看看CNN和LSTM。会写一点学习笔记啥的（乱立flag）了，溜了溜了🤦‍♀️]]></content>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[First Blog]]></title>
    <url>%2F2018%2F03%2F15%2FFirst%20Blog%2F</url>
    <content type="text"><![CDATA[With great power comes great responsibility. First Blog折腾了两天，Hexo框架➕GitHub部署➕域名解析终于简单搭起来了一个博客。很早就想用博客记录一下平时学习上课上遇到的问题👻，现在正好趁着实验室没什么事情玩一玩，希望自己能够坚持下去吧嘻嘻（狗头 自己脑补）。]]></content>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
</search>
